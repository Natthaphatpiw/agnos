# ‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏°‡∏£‡∏∞‡∏ö‡∏ö Symptom Recommendation Engine

‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏â‡∏ö‡∏±‡∏ö‡∏ô‡∏µ‡πâ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏°‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î ‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏Å‡∏≤‡∏£ train ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏õ‡∏à‡∏ô‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á API

---

## ‡∏™‡∏≤‡∏£‡∏ö‡∏±‡∏ç

1. [‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏£‡∏∞‡∏ö‡∏ö](#‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏£‡∏∞‡∏ö‡∏ö)
2. [‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏• ML](#‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏•-ml)
3. [‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ Training](#‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£-training)
4. [‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏° Backend API](#‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏°-backend-api)
5. [Data Flow ‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î](#data-flow-‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î)
6. [‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á](#‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á)

---

## ‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏£‡∏∞‡∏ö‡∏ö

### ‡∏à‡∏∏‡∏î‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå
‡∏£‡∏∞‡∏ö‡∏ö‡∏ô‡∏µ‡πâ‡∏ñ‡∏π‡∏Å‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠ **‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ‡∏ï‡πà‡∏≠‡πÑ‡∏õ** ‡πÇ‡∏î‡∏¢‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:
- **Demographics**: ‡πÄ‡∏û‡∏® (male/female) ‡πÅ‡∏•‡∏∞‡∏≠‡∏≤‡∏¢‡∏∏ (0-120 ‡∏õ‡∏µ)
- **Initial Symptoms**: ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡∏°‡∏µ (‡πÄ‡∏ä‡πà‡∏ô ‡πÑ‡∏≠, ‡∏õ‡∏ß‡∏î‡∏ó‡πâ‡∏≠‡∏á)

‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö Netflix ‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏´‡∏ô‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ö‡∏ä‡∏° ‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏≤‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï

### ‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏´‡∏•‡∏±‡∏Å
- **PyTorch**: ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á Deep Learning Model
- **PyTorch Geometric**: ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Graph Neural Networks (GAT)
- **FastAPI**: ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á REST API
- **Pandas/NumPy**: ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

---

## ‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏• ML

### üéØ ‡πÇ‡∏°‡πÄ‡∏î‡∏• Hybrid ‡πÅ‡∏ö‡∏ö 3 ‡∏™‡∏≤‡∏¢ (Triple-Path Architecture)

‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ô‡∏µ‡πâ‡∏£‡∏ß‡∏°‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ 3 ‡πÅ‡∏ö‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô:

```
Input (Gender, Age, Symptoms)
         ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Preprocess‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                                 ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  CF   ‚îÇ  ‚îÇ     CB      ‚îÇ  ‚îÇ  Graph  ‚îÇ
‚îÇ(NCF)  ‚îÇ  ‚îÇ  (Cosine)   ‚îÇ  ‚îÇ  (GAT)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ             ‚îÇ              ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚Üì
         Weighted Fusion
         (0.4 + 0.4 + 0.2)
              ‚Üì
         Top-K Rankings
              ‚Üì
        Recommendations
```

---

### 1Ô∏è‚É£ Collaborative Filtering (CF) - Neural Collaborative Filtering (NCF)

**‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£**: ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡∏ó‡∏µ‡πà‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô ‡πÄ‡∏ä‡πà‡∏ô ‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô‡∏°‡∏±‡∏Å‡∏°‡∏µ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô

#### Architecture:

```python
# NCF = GMF + MLP

# 1. GMF (Generalized Matrix Factorization)
patient_embed = Embedding(patient_idx)      # [batch, 128]
symptom_embed = Embedding(symptom_idx)      # [batch, 128]
gmf_vector = patient_embed * symptom_embed  # Element-wise product
gmf_score = Linear(gmf_vector)              # [batch, 1]

# 2. MLP (Multi-Layer Perceptron)
mlp_input = Concat([patient_embed, symptom_embed, demographics])  # [batch, 272]
mlp_hidden = Linear(272 ‚Üí 256) ‚Üí ReLU ‚Üí BatchNorm ‚Üí Dropout
           ‚Üí Linear(256 ‚Üí 128) ‚Üí ReLU ‚Üí BatchNorm ‚Üí Dropout
           ‚Üí Linear(128 ‚Üí 64)  ‚Üí ReLU ‚Üí BatchNorm ‚Üí Dropout
           ‚Üí Linear(64 ‚Üí 32)   ‚Üí ReLU ‚Üí BatchNorm ‚Üí Dropout
mlp_score = Linear(32 ‚Üí 1)

# 3. Combine
cf_score = gmf_score + mlp_score
```

**Input Features**:
- **Patient Embedding**: ‡πÄ‡∏•‡∏Ç index ‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢ (‡πÉ‡∏ä‡πâ embedding 128 ‡∏°‡∏¥‡∏ï‡∏¥)
- **Symptom Embedding**: ‡πÄ‡∏•‡∏Ç index ‡∏Ç‡∏≠‡∏á‡∏≠‡∏≤‡∏Å‡∏≤‡∏£ (‡πÉ‡∏ä‡πâ embedding 128 ‡∏°‡∏¥‡∏ï‡∏¥)
- **Demographics Embedding**: ‡πÄ‡∏û‡∏® + ‡∏≠‡∏≤‡∏¢‡∏∏ (encode ‡πÄ‡∏õ‡πá‡∏ô 16 ‡∏°‡∏¥‡∏ï‡∏¥)

**‡∏Ç‡πâ‡∏≠‡∏î‡∏µ**:
- ‡∏à‡∏±‡∏ö pattern ‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏ú‡πà‡∏≤‡∏ô MLP
- GMF ‡∏ä‡πà‡∏ß‡∏¢‡∏à‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡πÅ‡∏ö‡∏ö linear
- ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ user-item interaction ‡∏°‡∏≤‡∏Å

---

### 2Ô∏è‚É£ Content-Based Filtering (CB)

**‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£**: ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Ñ‡∏•‡∏∂‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡∏ú‡πà‡∏≤‡∏ô embeddings

#### Algorithm:

```python
# 1. Embed query symptoms
query_embeds = SymptomEmbedding(query_symptoms)  # [batch, num_query, 128]
query_avg = Mean(query_embeds, dim=1)            # [batch, 1, 128]

# 2. Embed candidate symptoms
candidate_embeds = SymptomEmbedding(candidates)  # [batch, num_candidates, 128]

# 3. Cosine Similarity
query_norm = Normalize(query_avg, p=2)           # L2 normalization
candidate_norm = Normalize(candidate_embeds, p=2)

cb_scores = CosineSim(query_norm, candidate_norm)
          = (query_norm * candidate_norm).sum(dim=-1)  # [batch, num_candidates]
```

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**:
```
Query: ["‡πÑ‡∏≠"]
  ‚Üí Embedding: [0.23, -0.45, 0.12, ...]

Candidates:
  "‡πÄ‡∏™‡∏°‡∏´‡∏∞"    ‚Üí Embedding: [0.25, -0.40, 0.15, ...] ‚Üí Cosine = 0.89 (‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏°‡∏≤‡∏Å)
  "‡∏ô‡πâ‡∏≥‡∏°‡∏π‡∏Å‡πÑ‡∏´‡∏•" ‚Üí Embedding: [0.20, -0.38, 0.10, ...] ‚Üí Cosine = 0.82
  "‡∏õ‡∏ß‡∏î‡∏ó‡πâ‡∏≠‡∏á"  ‚Üí Embedding: [-0.10, 0.30, -0.25, ...] ‚Üí Cosine = 0.15 (‡πÑ‡∏°‡πà‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢)
```

**‡∏Ç‡πâ‡∏≠‡∏î‡∏µ**:
- ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡πÅ‡∏°‡πâ‡∏Å‡∏±‡∏ö‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡πÉ‡∏´‡∏°‡πà (Cold Start Problem)
- Interpretable - ‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÑ‡∏´‡∏ô‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô
- Fast inference

---

### 3Ô∏è‚É£ Graph Neural Network (GAT - Graph Attention Network)

**‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£**: ‡∏™‡∏£‡πâ‡∏≤‡∏á graph ‡∏ó‡∏µ‡πà nodes ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏≤‡∏Å‡∏≤‡∏£ ‡πÅ‡∏•‡∏∞ edges ‡∏Ñ‡∏∑‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡∏¥‡∏î‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô (co-occurrence)

#### Graph Construction:

```python
# ‡∏™‡∏£‡πâ‡∏≤‡∏á Graph ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• training
G = NetworkX.Graph()
G.add_nodes(all_symptoms)  # 338 nodes

# ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢
for patient in patients:
    symptoms = patient.yes_symptoms  # ‡πÄ‡∏ä‡πà‡∏ô ["‡πÑ‡∏≠", "‡πÄ‡∏™‡∏°‡∏´‡∏∞", "‡πÄ‡∏à‡πá‡∏ö‡∏Ñ‡∏≠"]

    # ‡πÄ‡∏û‡∏¥‡πà‡∏° edge ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏π‡πà
    for (symptom_i, symptom_j) in combinations(symptoms, 2):
        edge = (symptom_i, symptom_j)
        edge_weights[edge] += 1  # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô

# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á edges:
# ("‡πÑ‡∏≠", "‡πÄ‡∏™‡∏°‡∏´‡∏∞") ‚Üí weight = 156 (‡πÄ‡∏Å‡∏¥‡∏î‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô 156 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á)
# ("‡πÑ‡∏≠", "‡πÄ‡∏à‡πá‡∏ö‡∏Ñ‡∏≠") ‚Üí weight = 203
# ("‡∏õ‡∏ß‡∏î‡∏ó‡πâ‡∏≠‡∏á", "‡∏Ñ‡∏•‡∏∑‡πà‡∏ô‡πÑ‡∏™‡πâ") ‚Üí weight = 89
```

#### GAT Architecture:

```python
# Input: Symptom embeddings X ‚àà R^(338 √ó 128)
X = SymptomEmbeddings.weight  # [338, 128]

# Layer 1: Multi-head attention (4 heads)
H1 = GAT_Layer1(X, edge_index, heads=4)  # [338, 128*4]
H1 = ELU(H1)
H1 = Dropout(H1, p=0.3)

# Layer 2: Single-head attention
H2 = GAT_Layer2(H1, edge_index, heads=1)  # [338, 128]

# H2 ‡∏Ñ‡∏∑‡∏≠ symptom embeddings ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å propagate ‡∏ú‡πà‡∏≤‡∏ô graph
```

**Attention Mechanism**:

‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ node (‡∏≠‡∏≤‡∏Å‡∏≤‡∏£), GAT ‡∏à‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì attention weights ‡∏Å‡∏±‡∏ö neighbors:

```python
# ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö node "‡πÑ‡∏≠"
neighbors = ["‡πÄ‡∏™‡∏°‡∏´‡∏∞", "‡πÄ‡∏à‡πá‡∏ö‡∏Ñ‡∏≠", "‡∏ô‡πâ‡∏≥‡∏°‡∏π‡∏Å‡πÑ‡∏´‡∏•", ...]

# ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì attention scores
for neighbor in neighbors:
    # Attention coefficient
    alpha = softmax(
        LeakyReLU(
            W1 @ embed("‡πÑ‡∏≠") + W2 @ embed(neighbor)
        )
    )

# Update embedding ‡∏Ç‡∏≠‡∏á "‡πÑ‡∏≠"
new_embed("‡πÑ‡∏≠") = Œ£ (alpha_neighbor √ó embed(neighbor))
```

**‡∏Ç‡πâ‡∏≠‡∏î‡∏µ**:
- ‡∏à‡∏±‡∏ö **co-occurrence patterns** ‡πÑ‡∏î‡πâ‡∏î‡∏µ ‡πÄ‡∏ä‡πà‡∏ô "‡πÑ‡∏≠" ‡∏°‡∏±‡∏Å‡∏°‡∏≤‡∏Å‡∏±‡∏ö "‡πÄ‡∏™‡∏°‡∏´‡∏∞"
- Attention weights ‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÑ‡∏´‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Å‡∏±‡∏ö‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÑ‡∏´‡∏ô
- ‡πÑ‡∏°‡πà‡∏à‡∏≥‡∏Å‡∏±‡∏î‡πÅ‡∏Ñ‡πà direct connections (‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏î‡πâ‡∏´‡∏•‡∏≤‡∏¢ hops)

---

### üîÄ Fusion Strategy

‡∏£‡∏ß‡∏° scores ‡∏à‡∏≤‡∏Å 3 ‡∏™‡∏≤‡∏¢‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô:

```python
# 1. Normalize scores to [0, 1]
cf_norm = Sigmoid(cf_scores)              # CF score
cb_norm = (cb_scores + 1) / 2            # Cosine [-1,1] ‚Üí [0,1]
graph_norm = (graph_scores + 1) / 2      # Cosine [-1,1] ‚Üí [0,1]

# 2. Learnable fusion weights (‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ó‡∏µ‡πà [0.4, 0.4, 0.2])
weights = [w_cf, w_cb, w_graph]
weights = Softmax(weights)  # ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏£‡∏ß‡∏° = 1

# 3. Weighted sum
final_scores = (
    weights[0] * cf_norm +      # 40% ‡∏à‡∏≤‡∏Å CF
    weights[1] * cb_norm +      # 40% ‡∏à‡∏≤‡∏Å Content-Based
    weights[2] * graph_norm     # 20% ‡∏à‡∏≤‡∏Å Graph
)

# 4. Top-K selection
top_k_indices = ArgTopK(final_scores, k=5)
recommendations = [idx_to_symptom[idx] for idx in top_k_indices]
```

**‡∏ó‡∏≥‡πÑ‡∏°‡πÉ‡∏ä‡πâ weights ‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ?**
- **CF (40%)**: ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡∏à‡∏£‡∏¥‡∏á
- **CB (40%)**: ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏û‡∏≠‡πÜ ‡∏Å‡∏±‡∏ô ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏à‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏ï‡∏≤‡∏°‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏î‡πâ‡∏î‡∏µ
- **Graph (20%)**: ‡πÄ‡∏õ‡πá‡∏ô supplement ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏° context ‡∏Ç‡∏≠‡∏á co-occurrence

**Learnable Weights**: ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏õ‡∏£‡∏±‡∏ö‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á training ‡∏ú‡πà‡∏≤‡∏ô `nn.Parameter`

---

## ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ Training

### üìä Data Preprocessing (‡πÑ‡∏ü‡∏•‡πå `train.py`)

#### Step 1: ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞ Parse ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

```python
# ‡πÇ‡∏´‡∏•‡∏î CSV
df = pd.read_csv('AI symptom picker data.csv')
# Columns: gender, age, summary (JSON), search_term

# Parse JSON summary
for row in df:
    summary = json.loads(row['summary'])

    # Extract yes_symptoms
    yes_symptoms = []
    for sym_obj in summary['yes_symptoms']:
        symptom = sym_obj['text']           # ‡πÄ‡∏ä‡πà‡∏ô "‡πÑ‡∏≠"
        answers = sym_obj['answers']        # ‡πÄ‡∏ä‡πà‡∏ô ["‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤ 1-3 ‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå"]

        # Parse severity ‡∏à‡∏≤‡∏Å answers
        severity = parse_severity(answers)  # 1.0 - 3.0

        yes_symptoms.append(symptom)
        symptom_details[symptom] = {
            'answers': answers,
            'severity': severity
        }

    # Extract no_symptoms
    no_symptoms = [sym['text'] for sym in summary['no_symptoms']]

    # Extract from search_term
    search_symptoms = row['search_term'].split(',')
```

**Severity Parsing**:

```python
def parse_severity(answers):
    """‡πÅ‡∏õ‡∏•‡∏á text ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô severity score"""
    severity_map = {
        '‡∏õ‡∏ß‡∏î‡∏à‡∏ô‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ': 3.0,           # ‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á‡∏°‡∏≤‡∏Å
        '‡∏õ‡∏ß‡∏î‡∏à‡∏ô‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°‡πÉ‡∏î‡πÉ‡∏î‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢': 3.0,
        '‡∏õ‡∏ß‡∏î‡∏°‡∏≤‡∏Å': 2.5,
        '‡∏õ‡∏ß‡∏î‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á': 2.0,                        # ‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á
        '‡∏™‡πà‡∏á‡∏ú‡∏•‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏¥‡∏à‡∏ß‡∏±‡∏ï‡∏£‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏ß‡∏±‡∏ô‡∏ö‡πâ‡∏≤‡∏á': 1.5,
        '‡∏õ‡∏ß‡∏î‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢': 1.0,                       # ‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
        '‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢': 1.0,
    }

    for pattern, score in severity_map.items():
        if pattern in ' '.join(answers):
            return score

    return 1.0  # Default
```

---

#### Step 2: ‡∏™‡∏£‡πâ‡∏≤‡∏á Vocabulary

```python
# ‡∏£‡∏ß‡∏°‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏û‡∏ö
all_symptoms = set()

for patient in patients:
    all_symptoms.update(patient['yes_symptoms'])
    all_symptoms.update(patient['no_symptoms'])
    all_symptoms.update(patient['search_symptoms'])

# ‡∏™‡∏£‡πâ‡∏≤‡∏á mapping
symptom_to_idx = {sym: idx for idx, sym in enumerate(sorted(all_symptoms))}
idx_to_symptom = {idx: sym for sym, idx in symptom_to_idx.items()}

# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:
# symptom_to_idx = {
#     "‡πÑ‡∏≠": 0,
#     "‡πÄ‡∏™‡∏°‡∏´‡∏∞": 1,
#     "‡∏õ‡∏ß‡∏î‡∏ó‡πâ‡∏≠‡∏á": 2,
#     ...
# }
```

**‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå**: Vocabulary size = 338 symptoms

---

#### Step 3: ‡∏™‡∏£‡πâ‡∏≤‡∏á Interaction Matrix

```python
# Matrix: [num_patients √ó num_symptoms]
interaction_matrix = np.zeros((1000, 338), dtype=np.float32)

for patient_idx, patient in enumerate(patients):
    # Yes symptoms ‚Üí positive weight (weighted by severity)
    for symptom in patient['yes_symptoms']:
        sym_idx = symptom_to_idx[symptom]
        severity = patient['symptom_details'][symptom]['severity']
        interaction_matrix[patient_idx, sym_idx] = severity  # 1.0 - 3.0

    # No symptoms ‚Üí negative weight
    for symptom in patient['no_symptoms']:
        sym_idx = symptom_to_idx[symptom]
        interaction_matrix[patient_idx, sym_idx] = -1.0

# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á row ‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢ #5:
# [0, 2.5, 0, 1.0, 0, ..., -1.0, 0, ...]
#  ^   ^      ^             ^
#  |   |      |             |
#  |   |      |             no_symptom
#  |   |      yes_symptom (severity=1.0)
#  |   yes_symptom (severity=2.5)
#  ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ô‡∏µ‡πâ
```

---

#### Step 4: ‡∏™‡∏£‡πâ‡∏≤‡∏á Co-occurrence Graph

```python
# ‡∏™‡∏£‡πâ‡∏≤‡∏á NetworkX graph
G = nx.Graph()
G.add_nodes_from(range(338))  # 338 symptoms

edge_weights = defaultdict(float)

for patient in patients:
    symptoms = patient['yes_symptoms']
    symptom_indices = [symptom_to_idx[s] for s in symptoms]

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á edge ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏π‡πà‡∏Ç‡∏≠‡∏á‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô
    for i in range(len(symptom_indices)):
        for j in range(i+1, len(symptom_indices)):
            edge = tuple(sorted([symptom_indices[i], symptom_indices[j]]))
            edge_weights[edge] += 1.0  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å

# Add edges
for (u, v), weight in edge_weights.items():
    G.add_edge(u, v, weight=weight)

# ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô PyTorch Geometric format
edge_index = []
edge_attr = []

for u, v, data in G.edges(data=True):
    # Undirected ‚Üí ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ó‡∏±‡πâ‡∏á 2 ‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á
    edge_index.append([u, v])
    edge_index.append([v, u])

    weight = data['weight']
    edge_attr.append([weight])
    edge_attr.append([weight])

edge_index = torch.tensor(edge_index).t()  # [2, num_edges]
edge_attr = torch.tensor(edge_attr)        # [num_edges, 1]

graph_data = Data(
    edge_index=edge_index,
    edge_attr=edge_attr,
    num_nodes=338
)
```

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á Graph**:
```
Nodes: 338 (‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)
Edges: ~5,000-10,000 (‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö co-occurrence)

‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á edges:
  (0, 1): weight=156    # "‡πÑ‡∏≠" ‚Üî "‡πÄ‡∏™‡∏°‡∏´‡∏∞" (‡πÄ‡∏Å‡∏¥‡∏î‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô 156 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á)
  (0, 45): weight=203   # "‡πÑ‡∏≠" ‚Üî "‡πÄ‡∏à‡πá‡∏ö‡∏Ñ‡∏≠"
  (2, 89): weight=89    # "‡∏õ‡∏ß‡∏î‡∏ó‡πâ‡∏≠‡∏á" ‚Üî "‡∏Ñ‡∏•‡∏∑‡πà‡∏ô‡πÑ‡∏™‡πâ"
```

---

#### Step 5: Encode Demographics

```python
age_bins = [0, 20, 30, 40, 100]  # 4 age groups

for patient in patients:
    # Gender one-hot encoding
    gender_vec = np.zeros(2)
    gender_vec[0 if patient['gender'] == 'male' else 1] = 1.0
    # male: [1, 0], female: [0, 1]

    # Age bin one-hot encoding
    age_bin_vec = np.zeros(4)
    bin_idx = np.digitize(patient['age'], age_bins) - 1
    bin_idx = max(0, min(3, bin_idx))  # Clip to [0, 3]
    age_bin_vec[bin_idx] = 1.0

    # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:
    # age=26 ‚Üí bin_idx=1 (20-30) ‚Üí [0, 1, 0, 0]
    # age=55 ‚Üí bin_idx=3 (40+)   ‚Üí [0, 0, 0, 1]

    patient['gender_vec'] = gender_vec
    patient['age_bin_vec'] = age_bin_vec
```

**Demographics Encoding**:
- Gender (2D) + Age Bin (4D) = **6D input**
- ‡∏ú‡πà‡∏≤‡∏ô Linear layer ‚Üí **16D demographics embedding**

---

### üéì Training Loop

#### Dataset Class

```python
class SymptomDataset(Dataset):
    def __getitem__(self, idx):
        patient = self.data[idx]

        # Split symptoms: query vs target
        pos_symptoms = patient['yes_symptoms']

        if len(pos_symptoms) > 1:
            split_idx = len(pos_symptoms) // 2
            query_symptoms = pos_symptoms[:split_idx]      # ‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô input
            target_symptoms = pos_symptoms[split_idx:]     # ‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô target
        else:
            query_symptoms = pos_symptoms
            target_symptoms = pos_symptoms

        # Encode to indices
        query_indices = [self.symptom_to_idx[s] for s in query_symptoms]
        target_indices = [self.symptom_to_idx[s] for s in target_symptoms]
        neg_indices = [self.symptom_to_idx[s] for s in patient['no_symptoms']]

        # Pad query to fixed length (10)
        query_indices += [0] * (10 - len(query_indices))

        # Create binary labels
        labels = torch.zeros(num_symptoms)  # [338]
        for idx in target_indices:
            labels[idx] = 1.0  # Positive symptoms

        return {
            'patient_idx': idx,
            'gender': patient['gender_vec'],
            'age_bin': patient['age_bin_vec'],
            'query_symptoms': torch.tensor(query_indices),
            'target_symptoms': target_indices,
            'neg_symptoms': neg_indices,
            'labels': labels
        }
```

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á batch**:
```
‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡∏°‡∏µ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£: ["‡πÑ‡∏≠", "‡πÄ‡∏™‡∏°‡∏´‡∏∞", "‡πÄ‡∏à‡πá‡∏ö‡∏Ñ‡∏≠", "‡∏ô‡πâ‡∏≥‡∏°‡∏π‡∏Å‡πÑ‡∏´‡∏•"]

Split:
  Query:  ["‡πÑ‡∏≠", "‡πÄ‡∏™‡∏°‡∏´‡∏∞"]              ‚Üí Input
  Target: ["‡πÄ‡∏à‡πá‡∏ö‡∏Ñ‡∏≠", "‡∏ô‡πâ‡∏≥‡∏°‡∏π‡∏Å‡πÑ‡∏´‡∏•"]     ‚Üí Ground truth

Labels: [0, 0, ..., 1, ..., 1, ...]  (1 ‡∏ó‡∏µ‡πà‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏Ç‡∏≠‡∏á target symptoms)
```

---

#### Training Function

```python
def train_epoch(model, dataloader, optimizer, criterion_bce, edge_index, device):
    model.train()
    total_loss = 0

    for batch in dataloader:
        # Move to device
        patient_idx = batch['patient_idx'].to(device)      # [32]
        gender = batch['gender'].to(device)                # [32, 2]
        age_bin = batch['age_bin'].to(device)              # [32, 4]
        query_symptoms = batch['query_symptoms'].to(device)  # [32, 10]
        labels = batch['labels'].to(device)                # [32, 338]

        optimizer.zero_grad()

        # Forward pass
        scores = model(
            patient_idx=patient_idx,
            gender=gender,
            age_bin=age_bin,
            query_symptoms=query_symptoms,
            edge_index=edge_index
        )  # ‚Üí [32, 338]

        # Compute loss
        loss = criterion_bce(scores, labels)  # Binary Cross-Entropy

        # Backward
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()

        total_loss += loss.item()

    return total_loss / len(dataloader)
```

**Loss Function**: Binary Cross-Entropy (BCE)

```python
BCE = -[y * log(≈∑) + (1-y) * log(1-≈∑)]

# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:
# Target: y = [0, 1, 0, 1, 0]
# Pred:   ≈∑ = [0.1, 0.9, 0.2, 0.8, 0.15]
#
# Loss = -(
#     0*log(0.1) + 1*log(0.9) +      # symptom 0 (negative, correct)
#     1*log(0.9) +                    # symptom 1 (positive, correct)
#     0*log(0.2) + 1*log(0.8) +      # ...
#     0*log(0.15)
# )
```

---

#### Evaluation Function

```python
def evaluate(model, dataloader, edge_index, device, k=5):
    model.eval()
    all_precisions = []
    all_recalls = []

    with torch.no_grad():
        for batch in dataloader:
            # Forward
            scores = model(...)  # [batch, 338]
            labels = batch['labels']

            for i in range(len(scores)):
                # Mask query symptoms (‡∏´‡πâ‡∏≤‡∏°‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà input ‡∏°‡∏≤)
                query_mask = (query_symptoms[i] != 0)
                scores[i][query_symptoms[i][query_mask]] = -float('inf')

                # Get top-K predictions
                top_k_indices = torch.topk(scores[i], k=5).indices
                true_indices = torch.where(labels[i] > 0)[0]

                # Calculate metrics
                tp = len(set(top_k_indices) & set(true_indices))
                precision = tp / k
                recall = tp / len(true_indices) if len(true_indices) > 0 else 0

                all_precisions.append(precision)
                all_recalls.append(recall)

    return np.mean(all_precisions), np.mean(all_recalls)
```

**Metrics**:
- **Precision@5**: ‡∏à‡∏≤‡∏Å 5 ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ ‡∏Å‡∏µ‡πà‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
  ```
  Precision@5 = (‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ô top-5) / 5
  ```

- **Recall@5**: ‡∏à‡∏≤‡∏Å‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÑ‡∏î‡πâ‡∏Å‡∏µ‡πà‡∏≠‡∏≤‡∏Å‡∏≤‡∏£
  ```
  Recall@5 = (‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ô top-5) / (‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)
  ```

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**:
```
Ground Truth: ["‡πÄ‡∏™‡∏°‡∏´‡∏∞", "‡πÄ‡∏à‡πá‡∏ö‡∏Ñ‡∏≠", "‡∏ô‡πâ‡∏≥‡∏°‡∏π‡∏Å‡πÑ‡∏´‡∏•"]  (3 ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£)
Predicted Top-5: ["‡πÄ‡∏™‡∏°‡∏´‡∏∞", "‡πÑ‡∏Ç‡πâ", "‡πÄ‡∏à‡πá‡∏ö‡∏Ñ‡∏≠", "‡∏õ‡∏ß‡∏î‡∏®‡∏µ‡∏£‡∏©‡∏∞", "‡∏Ñ‡∏•‡∏∑‡πà‡∏ô‡πÑ‡∏™‡πâ"]

‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á: ["‡πÄ‡∏™‡∏°‡∏´‡∏∞", "‡πÄ‡∏à‡πá‡∏ö‡∏Ñ‡∏≠"]  (2 ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£)

Precision@5 = 2/5 = 0.40
Recall@5 = 2/3 = 0.67
```

---

#### Main Training Loop

```python
# Hyperparameters
EPOCHS = 50
BATCH_SIZE = 32
LR = 1e-3

# Initialize
model = SymptomRecommender(num_symptoms=338, num_patients=1000)
optimizer = AdamW(model.parameters(), lr=LR, weight_decay=1e-5)
scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)
criterion = nn.BCELoss()

best_val_recall = 0.0

for epoch in range(EPOCHS):
    # Train
    train_loss = train_epoch(model, train_loader, optimizer, criterion, edge_index, device)

    # Validate
    val_precision, val_recall = evaluate(model, val_loader, edge_index, device, k=5)

    # Update learning rate
    scheduler.step(val_recall)

    print(f"Epoch {epoch+1}/{EPOCHS}")
    print(f"  Train Loss: {train_loss:.4f}")
    print(f"  Val P@5: {val_precision:.4f}")
    print(f"  Val R@5: {val_recall:.4f}")

    # Save best model
    if val_recall > best_val_recall:
        best_val_recall = val_recall
        torch.save(model.state_dict(), 'model.pth')
        print(f"  ‚Üí Saved best model!")
```

**Learning Rate Scheduling**:
- ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô: `lr = 1e-3`
- ‡∏ñ‡πâ‡∏≤ Recall@5 ‡πÑ‡∏°‡πà‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡πÉ‡∏ô 5 epochs ‚Üí ‡∏•‡∏î lr ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏Ñ‡∏£‡∏∂‡πà‡∏á
- ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ model converge ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô

---

### üíæ Artifacts ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å Training

‡∏´‡∏•‡∏±‡∏á training ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏à‡∏∞‡πÑ‡∏î‡πâ‡πÑ‡∏ü‡∏•‡πå:

1. **`model.pth`**: Model weights (state_dict)
   - ‡∏Ç‡∏ô‡∏≤‡∏î: ~10-20 MB
   - ‡πÄ‡∏Å‡πá‡∏ö weights ‡∏Ç‡∏≠‡∏á‡∏ó‡∏∏‡∏Å layer

2. **`symptom_to_idx.pkl`**: Vocabulary mapping
   ```python
   {
       "‡πÑ‡∏≠": 0,
       "‡πÄ‡∏™‡∏°‡∏´‡∏∞": 1,
       "‡∏õ‡∏ß‡∏î‡∏ó‡πâ‡∏≠‡∏á": 2,
       ...
   }
   ```

3. **`idx_to_symptom.pkl`**: Reverse mapping
   ```python
   {
       0: "‡πÑ‡∏≠",
       1: "‡πÄ‡∏™‡∏°‡∏´‡∏∞",
       2: "‡∏õ‡∏ß‡∏î‡∏ó‡πâ‡∏≠‡∏á",
       ...
   }
   ```

4. **`age_bins.pkl`**: Age bin boundaries
   ```python
   [0, 20, 30, 40, 100]
   ```

5. **`graph.pt`**: Co-occurrence graph
   - PyTorch Geometric Data object
   - ‡πÄ‡∏Å‡πá‡∏ö edge_index ‡πÅ‡∏•‡∏∞ edge_attr

6. **`model_config.pkl`**: Model configuration
   ```python
   {
       'num_symptoms': 338,
       'num_patients': 1000,
       'symptom_embed_dim': 128
   }
   ```

---

## ‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏° Backend API

### üöÄ FastAPI Application (‡πÑ‡∏ü‡∏•‡πå `app.py`)

#### Startup Process

```python
@asynccontextmanager
async def lifespan(app: FastAPI):
    """‡πÇ‡∏´‡∏•‡∏î model ‡πÅ‡∏•‡∏∞ artifacts ‡πÄ‡∏°‡∏∑‡πà‡∏≠ server start"""
    global model, symptom_to_idx, idx_to_symptom, age_bins, edge_index, device

    # 1. ‡πÇ‡∏´‡∏•‡∏î artifacts
    with open('symptom_to_idx.pkl', 'rb') as f:
        symptom_to_idx = pickle.load(f)

    with open('idx_to_symptom.pkl', 'rb') as f:
        idx_to_symptom = pickle.load(f)

    with open('age_bins.pkl', 'rb') as f:
        age_bins = pickle.load(f)

    with open('model_config.pkl', 'rb') as f:
        model_config = pickle.load(f)

    graph_data = torch.load('graph.pt', weights_only=False)
    edge_index = graph_data.edge_index

    # 2. Initialize model
    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    model = SymptomRecommender(
        num_symptoms=model_config['num_symptoms'],
        num_patients=model_config['num_patients'],
        symptom_embed_dim=model_config['symptom_embed_dim']
    ).to(device)

    # 3. Load trained weights
    model.load_state_dict(torch.load('model.pth', map_location=device, weights_only=True))
    model.eval()  # Set to evaluation mode

    edge_index = edge_index.to(device)

    logger.info(f"Model loaded successfully on {device}")
    logger.info(f"Vocabulary size: {len(symptom_to_idx)}")

    yield  # Server is running

    # Cleanup (if needed)
    logger.info("Shutting down...")

app = FastAPI(lifespan=lifespan)
```

**‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠ start server**:
```
1. [Load] symptom_to_idx.pkl      ‚Üí RAM: ~50 KB
2. [Load] idx_to_symptom.pkl      ‚Üí RAM: ~50 KB
3. [Load] age_bins.pkl            ‚Üí RAM: ~1 KB
4. [Load] model_config.pkl        ‚Üí RAM: ~1 KB
5. [Load] graph.pt                ‚Üí RAM: ~5 MB
6. [Init] SymptomRecommender      ‚Üí RAM: ~20 MB
7. [Load] model.pth weights       ‚Üí RAM: ~20 MB
8. [Move] to device (CPU/GPU)

Total RAM: ~45 MB
Ready to serve requests! üöÄ
```

---

### üì° API Endpoints

#### 1. `POST /recommend` - ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏≠‡∏≤‡∏Å‡∏≤‡∏£

```python
@app.post("/recommend", response_model=RecommendResponse)
async def recommend_symptoms(request: RecommendRequest):
    # Validate input
    if request.gender.lower() not in ['male', 'female']:
        raise HTTPException(400, "Gender must be 'male' or 'female'")

    if not request.symptoms:
        raise HTTPException(400, "At least one symptom required")

    # Check vocabulary
    known_symptoms = []
    unknown_symptoms = []

    for symptom in request.symptoms:
        if symptom in symptom_to_idx:
            known_symptoms.append(symptom)
        else:
            unknown_symptoms.append(symptom)

    if not known_symptoms:
        raise HTTPException(
            400,
            f"None of the symptoms are in vocabulary. Unknown: {unknown_symptoms}"
        )

    # Get recommendations
    recommendations = model.recommend(
        patient_idx=0,  # Dummy index for inference
        gender=request.gender.lower(),
        age=request.age,
        query_symptoms=known_symptoms,
        symptom_to_idx=symptom_to_idx,
        idx_to_symptom=idx_to_symptom,
        edge_index=edge_index,
        age_bins=age_bins,
        top_k=request.top_k,
        device=device
    )

    return RecommendResponse(
        recommendations=recommendations,
        query_symptoms=known_symptoms,
        unknown_symptoms=unknown_symptoms
    )
```

#### 2. `GET /health` - Health check

```python
@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "model_loaded": model is not None,
        "device": str(device),
        "vocabulary_size": len(symptom_to_idx)
    }
```

#### 3. `GET /symptoms?limit=50` - ‡∏î‡∏π vocabulary

```python
@app.get("/symptoms")
async def list_symptoms(limit: int = 50):
    symptoms = list(symptom_to_idx.keys())[:limit]
    return {
        "symptoms": symptoms,
        "total": len(symptom_to_idx),
        "showing": len(symptoms)
    }
```

---

## Data Flow ‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î

### üîÑ Request ‚Üí Response Journey

#### Example Request:

```bash
curl -X POST http://localhost:8000/recommend \
  -H "Content-Type: application/json" \
  -d '{
    "gender": "male",
    "age": 26,
    "symptoms": ["‡πÑ‡∏≠", "‡πÄ‡∏™‡∏°‡∏´‡∏∞"],
    "top_k": 5
  }'
```

---

### Step-by-Step Execution:

#### **Step 1: Request Validation** ‚ö°Ô∏è ~0.1ms

```python
# FastAPI Pydantic validation
request = RecommendRequest(
    gender="male",      # ‚úì Valid
    age=26,             # ‚úì Valid (0-120)
    symptoms=["‡πÑ‡∏≠", "‡πÄ‡∏™‡∏°‡∏´‡∏∞"],  # ‚úì Valid list
    top_k=5             # ‚úì Valid (1-20)
)

# Check gender
if request.gender not in ['male', 'female']:
    raise HTTPException(400, "Invalid gender")

# Check symptoms list
if not request.symptoms:
    raise HTTPException(400, "No symptoms provided")
```

---

#### **Step 2: Vocabulary Lookup** ‚ö°Ô∏è ~0.1ms

```python
known_symptoms = []
unknown_symptoms = []

for symptom in request.symptoms:  # ["‡πÑ‡∏≠", "‡πÄ‡∏™‡∏°‡∏´‡∏∞"]
    if symptom in symptom_to_idx:
        known_symptoms.append(symptom)
    else:
        unknown_symptoms.append(symptom)

# Result:
# known_symptoms = ["‡πÑ‡∏≠", "‡πÄ‡∏™‡∏°‡∏´‡∏∞"]
# unknown_symptoms = []
```

---

#### **Step 3: Encode Demographics** ‚ö°Ô∏è ~0.2ms

```python
# 3.1 Gender encoding
gender_vec = torch.zeros(1, 2, device=device)  # [1, 2]
gender_vec[0, 0 if gender == 'male' else 1] = 1.0

# male ‚Üí [1.0, 0.0]
# Result: [[1.0, 0.0]]

# 3.2 Age bin encoding
age = 26
age_bin_vec = torch.zeros(1, 4, device=device)  # [1, 4]
bin_idx = np.digitize(age, age_bins) - 1
# age_bins = [0, 20, 30, 40, 100]
# 26 falls in bin [20, 30) ‚Üí bin_idx = 1

age_bin_vec[0, bin_idx] = 1.0
# Result: [[0.0, 1.0, 0.0, 0.0]]
```

---

#### **Step 4: Encode Query Symptoms** ‚ö°Ô∏è ~0.3ms

```python
query_symptoms = ["‡πÑ‡∏≠", "‡πÄ‡∏™‡∏°‡∏´‡∏∞"]
query_indices = []

for symptom in query_symptoms:
    idx = symptom_to_idx[symptom]
    query_indices.append(idx)

# symptom_to_idx = {"‡πÑ‡∏≠": 0, "‡πÄ‡∏™‡∏°‡∏´‡∏∞": 1, ...}
# Result: query_indices = [0, 1]

query_tensor = torch.tensor([query_indices], device=device)
# Result: [[0, 1]]  shape=[1, 2]
```

---

#### **Step 5: Model Forward Pass** ‚ö°Ô∏è ~20-50ms (CPU) / ~5-10ms (GPU)

```python
# 5.1 Demographics Encoding
demog_input = torch.cat([gender_vec, age_bin_vec], dim=-1)  # [1, 6]
demog_embed = model.demog_encoder(demog_input)               # [1, 16]

# Linear(6 ‚Üí 16) + ReLU + Dropout
# Result: [[0.23, -0.45, 0.12, ..., 0.67]]  (16 values)
```

```python
# 5.2 Query Symptom Embeddings
query_embeds = model.symptom_embeddings(query_tensor)  # [1, 2, 128]

# query_tensor = [[0, 1]]  (‡πÑ‡∏≠, ‡πÄ‡∏™‡∏°‡∏´‡∏∞)
# Result shape: [1, 2, 128]
# [
#   [
#     [0.23, -0.45, 0.12, ..., 0.89],  # embedding ‡∏Ç‡∏≠‡∏á "‡πÑ‡∏≠"
#     [0.25, -0.40, 0.15, ..., 0.85]   # embedding ‡∏Ç‡∏≠‡∏á "‡πÄ‡∏™‡∏°‡∏´‡∏∞"
#   ]
# ]
```

```python
# 5.3 GAT Propagation
gat_embeddings = model.forward_gat(edge_index)  # [338, 128]

# Layer 1: Multi-head attention
x = model.symptom_embeddings.weight  # [338, 128]
h1 = model.gat1(x, edge_index)       # [338, 512] (4 heads √ó 128)
h1 = F.elu(h1)
h1 = F.dropout(h1, 0.3)

# Layer 2: Single-head attention
gat_embeddings = model.gat2(h1, edge_index)  # [338, 128]

# ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÑ‡∏î‡πâ embedding ‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô graph propagation ‡πÅ‡∏•‡πâ‡∏ß
```

```python
# 5.4 Generate Candidate Symptoms
candidate_indices = [idx for idx in range(338) if idx not in [0, 1]]
# ‡πÑ‡∏°‡πà‡πÄ‡∏≠‡∏≤‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà query ‡∏°‡∏≤ (‡πÑ‡∏≠=0, ‡πÄ‡∏™‡∏°‡∏´‡∏∞=1)
# candidate_indices = [2, 3, 4, ..., 337]  (336 ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£)

candidate_tensor = torch.tensor([candidate_indices], device=device)  # [1, 336]
```

```python
# 5.5 Compute CF Scores
patient_idx_tensor = torch.tensor([0], device=device)  # Dummy patient

# Expand for all candidates
patient_idx_expanded = patient_idx_tensor.unsqueeze(1).expand(-1, 336).reshape(-1)  # [336]
demog_embed_expanded = demog_embed.unsqueeze(1).expand(-1, 336, -1).reshape(-1, 16)  # [336, 16]
candidate_flat = candidate_tensor.reshape(-1)  # [336]

# NCF forward
patient_embed = model.patient_embeddings(patient_idx_expanded)  # [336, 128]
symptom_embed = model.symptom_embeddings(candidate_flat)        # [336, 128]

# GMF
gmf_vector = patient_embed * symptom_embed   # [336, 128]
gmf_score = model.gmf_linear(gmf_vector)     # [336, 1]

# MLP
mlp_input = torch.cat([patient_embed, symptom_embed, demog_embed_expanded], dim=-1)  # [336, 272]
mlp_score = model.mlp(mlp_input)  # [336, 1]

# Combine
cf_scores = gmf_score + mlp_score  # [336, 1]
cf_scores = cf_scores.reshape(1, 336)  # [1, 336]
```

```python
# 5.6 Compute CB Scores
query_avg = query_embeds.mean(dim=1, keepdim=True)  # [1, 1, 128]
# Average of ["‡πÑ‡∏≠", "‡πÄ‡∏™‡∏°‡∏´‡∏∞"] embeddings

candidate_embeds = model.symptom_embeddings(candidate_tensor)  # [1, 336, 128]

# Cosine similarity
query_norm = F.normalize(query_avg, p=2, dim=-1)            # [1, 1, 128]
candidate_norm = F.normalize(candidate_embeds, p=2, dim=-1)  # [1, 336, 128]

cb_scores = (query_norm * candidate_norm).sum(dim=-1)  # [1, 336]
```

```python
# 5.7 Compute Graph Scores
query_gat_embeds = gat_embeddings[query_tensor]  # [1, 2, 128]
query_gat_avg = query_gat_embeds.mean(dim=1, keepdim=True)  # [1, 1, 128]

candidate_gat_embeds = gat_embeddings[candidate_tensor]  # [1, 336, 128]

query_gat_norm = F.normalize(query_gat_avg, p=2, dim=-1)
candidate_gat_norm = F.normalize(candidate_gat_embeds, p=2, dim=-1)

graph_scores = (query_gat_norm * candidate_gat_norm).sum(dim=-1)  # [1, 336]
```

```python
# 5.8 Fusion
cf_norm = torch.sigmoid(cf_scores)        # [0, 1]
cb_norm = (cb_scores + 1) / 2             # [-1, 1] ‚Üí [0, 1]
graph_norm = (graph_scores + 1) / 2       # [-1, 1] ‚Üí [0, 1]

# Learnable weights
weights = F.softmax(model.fusion_weights, dim=0)  # [0.4, 0.4, 0.2]

final_scores = (
    weights[0] * cf_norm +
    weights[1] * cb_norm +
    weights[2] * graph_norm
)  # [1, 336]

# Example values:
# Symptom "‡πÄ‡∏à‡πá‡∏ö‡∏Ñ‡∏≠" (idx=45):
#   CF:    0.82
#   CB:    0.75
#   Graph: 0.68
#   Final: 0.4*0.82 + 0.4*0.75 + 0.2*0.68 = 0.764
```

---

#### **Step 6: Top-K Selection** ‚ö°Ô∏è ~0.5ms

```python
top_k = 5
top_scores, top_local_indices = torch.topk(final_scores[0], k=top_k)

# top_scores: [0.853, 0.829, 0.764, 0.721, 0.698]
# top_local_indices: [43, 12, 45, 89, 102]
#   (indices ‡πÉ‡∏ô candidate_indices array)

# Map back to actual symptom indices
top_indices = [candidate_indices[idx.item()] for idx in top_local_indices]
# top_indices: [45, 14, 47, 91, 104]

# Map to symptom names
recommendations = [idx_to_symptom[idx] for idx in top_indices]
# recommendations = ["‡πÄ‡∏à‡πá‡∏ö‡∏Ñ‡∏≠", "‡∏ô‡πâ‡∏≥‡∏°‡∏π‡∏Å‡πÑ‡∏´‡∏•", "‡πÑ‡∏Ç‡πâ", "‡∏õ‡∏ß‡∏î‡∏®‡∏µ‡∏£‡∏©‡∏∞", "‡∏Ñ‡∏±‡∏î‡∏à‡∏°‡∏π‡∏Å"]
```

---

#### **Step 7: Response Construction** ‚ö°Ô∏è ~0.1ms

```python
response = RecommendResponse(
    recommendations=["‡πÄ‡∏à‡πá‡∏ö‡∏Ñ‡∏≠", "‡∏ô‡πâ‡∏≥‡∏°‡∏π‡∏Å‡πÑ‡∏´‡∏•", "‡πÑ‡∏Ç‡πâ", "‡∏õ‡∏ß‡∏î‡∏®‡∏µ‡∏£‡∏©‡∏∞", "‡∏Ñ‡∏±‡∏î‡∏à‡∏°‡∏π‡∏Å"],
    query_symptoms=["‡πÑ‡∏≠", "‡πÄ‡∏™‡∏°‡∏´‡∏∞"],
    unknown_symptoms=[]
)

# Convert to JSON
response_json = {
    "recommendations": ["‡πÄ‡∏à‡πá‡∏ö‡∏Ñ‡∏≠", "‡∏ô‡πâ‡∏≥‡∏°‡∏π‡∏Å‡πÑ‡∏´‡∏•", "‡πÑ‡∏Ç‡πâ", "‡∏õ‡∏ß‡∏î‡∏®‡∏µ‡∏£‡∏©‡∏∞", "‡∏Ñ‡∏±‡∏î‡∏à‡∏°‡∏π‡∏Å"],
    "query_symptoms": ["‡πÑ‡∏≠", "‡πÄ‡∏™‡∏°‡∏´‡∏∞"],
    "unknown_symptoms": []
}
```

---

### ‚è±Ô∏è Total Latency Breakdown

| Step | Description | Time (CPU) | Time (GPU) |
|------|-------------|------------|------------|
| 1 | Request Validation | 0.1 ms | 0.1 ms |
| 2 | Vocabulary Lookup | 0.1 ms | 0.1 ms |
| 3 | Encode Demographics | 0.2 ms | 0.1 ms |
| 4 | Encode Query Symptoms | 0.3 ms | 0.2 ms |
| 5 | Model Forward Pass | 20-50 ms | 5-10 ms |
| 6 | Top-K Selection | 0.5 ms | 0.3 ms |
| 7 | Response Construction | 0.1 ms | 0.1 ms |
| **Total** | | **21-51 ms** | **6-11 ms** |

**Bottleneck**: Model forward pass (Áâπ„Å´ CF score computation)

---

## ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á

### üìù Scenario 1: ‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡∏ä‡∏≤‡∏¢‡∏≠‡∏≤‡∏¢‡∏∏ 26 ‡∏õ‡∏µ ‡∏°‡∏µ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£ "‡πÑ‡∏≠"

#### Request:
```json
{
  "gender": "male",
  "age": 26,
  "symptoms": ["‡πÑ‡∏≠"],
  "top_k": 5
}
```

#### Processing:

1. **Demographics Encoding**:
   - Gender: male ‚Üí `[1.0, 0.0]`
   - Age: 26 ‚Üí bin 1 (20-30) ‚Üí `[0, 1, 0, 0]`
   - Demographics embedding: `[0.23, -0.45, ..., 0.67]` (16D)

2. **Symptom Embedding**:
   - "‡πÑ‡∏≠" ‚Üí index 0 ‚Üí embedding `[0.23, -0.45, 0.12, ...]` (128D)

3. **Scoring** (top candidates):

   | Symptom | CF Score | CB Score | Graph Score | Final Score |
   |---------|----------|----------|-------------|-------------|
   | ‡πÄ‡∏™‡∏°‡∏´‡∏∞ | 0.89 | 0.92 | 0.88 | **0.897** |
   | ‡πÄ‡∏à‡πá‡∏ö‡∏Ñ‡∏≠ | 0.85 | 0.78 | 0.85 | **0.828** |
   | ‡∏ô‡πâ‡∏≥‡∏°‡∏π‡∏Å‡πÑ‡∏´‡∏• | 0.81 | 0.75 | 0.82 | **0.793** |
   | ‡πÑ‡∏Ç‡πâ | 0.76 | 0.72 | 0.79 | **0.755** |
   | ‡∏õ‡∏ß‡∏î‡∏®‡∏µ‡∏£‡∏©‡∏∞ | 0.71 | 0.68 | 0.70 | **0.697** |

4. **Why these recommendations?**
   - **‡πÄ‡∏™‡∏°‡∏´‡∏∞**: ‡∏°‡∏±‡∏Å‡πÄ‡∏Å‡∏¥‡∏î‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ö "‡πÑ‡∏≠" (co-occurrence ‡∏™‡∏π‡∏á), semantic similarity ‡∏™‡∏π‡∏á
   - **‡πÄ‡∏à‡πá‡∏ö‡∏Ñ‡∏≠**: ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏≤‡∏á‡πÄ‡∏î‡∏¥‡∏ô‡∏´‡∏≤‡∏¢‡πÉ‡∏à‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô
   - **‡∏ô‡πâ‡∏≥‡∏°‡∏π‡∏Å‡πÑ‡∏´‡∏•**: ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏´‡∏ß‡∏±‡∏î/‡πÑ‡∏Ç‡πâ‡∏´‡∏ß‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ö‡πÑ‡∏≠
   - **‡πÑ‡∏Ç‡πâ**: ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡πÄ‡∏ä‡∏∑‡πâ‡∏≠
   - **‡∏õ‡∏ß‡∏î‡∏®‡∏µ‡∏£‡∏©‡∏∞**: ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ

#### Response:
```json
{
  "recommendations": ["‡πÄ‡∏™‡∏°‡∏´‡∏∞", "‡πÄ‡∏à‡πá‡∏ö‡∏Ñ‡∏≠", "‡∏ô‡πâ‡∏≥‡∏°‡∏π‡∏Å‡πÑ‡∏´‡∏•", "‡πÑ‡∏Ç‡πâ", "‡∏õ‡∏ß‡∏î‡∏®‡∏µ‡∏£‡∏©‡∏∞"],
  "query_symptoms": ["‡πÑ‡∏≠"],
  "unknown_symptoms": []
}
```

---

### üìù Scenario 2: ‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡∏´‡∏ç‡∏¥‡∏á‡∏≠‡∏≤‡∏¢‡∏∏ 35 ‡∏õ‡∏µ ‡∏°‡∏µ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£ "‡∏õ‡∏ß‡∏î‡∏ó‡πâ‡∏≠‡∏á", "‡∏Ñ‡∏•‡∏∑‡πà‡∏ô‡πÑ‡∏™‡πâ"

#### Request:
```json
{
  "gender": "female",
  "age": 35,
  "symptoms": ["‡∏õ‡∏ß‡∏î‡∏ó‡πâ‡∏≠‡∏á", "‡∏Ñ‡∏•‡∏∑‡πà‡∏ô‡πÑ‡∏™‡πâ"],
  "top_k": 5
}
```

#### Processing:

1. **Demographics**:
   - Gender: female ‚Üí `[0.0, 1.0]`
   - Age: 35 ‚Üí bin 2 (30-40) ‚Üí `[0, 0, 1, 0]`

2. **Query Embeddings**:
   - "‡∏õ‡∏ß‡∏î‡∏ó‡πâ‡∏≠‡∏á" ‚Üí `[0.15, 0.42, -0.23, ...]`
   - "‡∏Ñ‡∏•‡∏∑‡πà‡∏ô‡πÑ‡∏™‡πâ" ‚Üí `[0.18, 0.38, -0.20, ...]`
   - Average: `[0.165, 0.40, -0.215, ...]`

3. **Top Recommendations**:

   | Symptom | Reason |
   |---------|--------|
   | ‡∏≠‡∏≤‡πÄ‡∏à‡∏µ‡∏¢‡∏ô | ‡∏°‡∏±‡∏Å‡πÄ‡∏Å‡∏¥‡∏î‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ö‡∏Ñ‡∏•‡∏∑‡πà‡∏ô‡πÑ‡∏™‡πâ (co-occurrence 89%) |
   | ‡∏ó‡πâ‡∏≠‡∏á‡πÄ‡∏™‡∏µ‡∏¢ | ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏≤‡∏á‡πÄ‡∏î‡∏¥‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô |
   | ‡∏à‡∏∏‡∏Å‡πÅ‡∏ô‡πà‡∏ô‡∏ó‡πâ‡∏≠‡∏á | semantic similarity ‡∏Å‡∏±‡∏ö‡∏õ‡∏ß‡∏î‡∏ó‡πâ‡∏≠‡∏á |
   | ‡πÑ‡∏Ç‡πâ | ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡∏ó‡∏≤‡∏á‡πÄ‡∏î‡∏¥‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£ |
   | ‡∏õ‡∏ß‡∏î‡∏®‡∏µ‡∏£‡∏©‡∏∞ | ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ |

#### Response:
```json
{
  "recommendations": ["‡∏≠‡∏≤‡πÄ‡∏à‡∏µ‡∏¢‡∏ô", "‡∏ó‡πâ‡∏≠‡∏á‡πÄ‡∏™‡∏µ‡∏¢", "‡∏à‡∏∏‡∏Å‡πÅ‡∏ô‡πà‡∏ô‡∏ó‡πâ‡∏≠‡∏á", "‡πÑ‡∏Ç‡πâ", "‡∏õ‡∏ß‡∏î‡∏®‡∏µ‡∏£‡∏©‡∏∞"],
  "query_symptoms": ["‡∏õ‡∏ß‡∏î‡∏ó‡πâ‡∏≠‡∏á", "‡∏Ñ‡∏•‡∏∑‡πà‡∏ô‡πÑ‡∏™‡πâ"],
  "unknown_symptoms": []
}
```

---

### üìù Scenario 3: Unknown Symptom Handling

#### Request:
```json
{
  "gender": "male",
  "age": 45,
  "symptoms": ["‡∏ó‡πâ‡∏≠‡∏á‡πÅ‡∏™‡∏ö", "‡∏õ‡∏ß‡∏î‡∏ó‡πâ‡∏≠‡∏á"],  // "‡∏ó‡πâ‡∏≠‡∏á‡πÅ‡∏™‡∏ö" ‡πÑ‡∏°‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô vocab
  "top_k": 5
}
```

#### Processing:

1. **Vocabulary Check**:
   - "‡∏ó‡πâ‡∏≠‡∏á‡πÅ‡∏™‡∏ö" ‚Üí ‚ùå Not in vocabulary
   - "‡∏õ‡∏ß‡∏î‡∏ó‡πâ‡∏≠‡∏á" ‚Üí ‚úÖ In vocabulary (index 42)

2. **Proceed with known symptoms**: `["‡∏õ‡∏ß‡∏î‡∏ó‡πâ‡∏≠‡∏á"]`

3. **Generate recommendations**: (same as normal)

#### Response:
```json
{
  "recommendations": ["‡∏Ñ‡∏•‡∏∑‡πà‡∏ô‡πÑ‡∏™‡πâ", "‡∏à‡∏∏‡∏Å‡πÅ‡∏ô‡πà‡∏ô‡∏ó‡πâ‡∏≠‡∏á", "‡∏≠‡∏≤‡πÄ‡∏à‡∏µ‡∏¢‡∏ô", "‡∏ó‡πâ‡∏≠‡∏á‡πÄ‡∏™‡∏µ‡∏¢", "‡∏ó‡πâ‡∏≠‡∏á‡∏≠‡∏∑‡∏î"],
  "query_symptoms": ["‡∏õ‡∏ß‡∏î‡∏ó‡πâ‡∏≠‡∏á"],
  "unknown_symptoms": ["‡∏ó‡πâ‡∏≠‡∏á‡πÅ‡∏™‡∏ö"]  // ‚ö†Ô∏è Warning: unknown symptom
}
```

---

### üìù Scenario 4: All Unknown Symptoms (Error)

#### Request:
```json
{
  "gender": "female",
  "age": 28,
  "symptoms": ["‡∏ó‡πâ‡∏≠‡∏á‡πÅ‡∏™‡∏ö", "‡∏õ‡∏ß‡∏î‡∏´‡∏±‡∏ß"],  // Both not in vocab
  "top_k": 5
}
```

#### Response:
```json
{
  "detail": "None of the provided symptoms are in the vocabulary. Unknown: ['‡∏ó‡πâ‡∏≠‡∏á‡πÅ‡∏™‡∏ö', '‡∏õ‡∏ß‡∏î‡∏´‡∏±‡∏ß']"
}
```
**HTTP Status**: 400 Bad Request

---

## ‡∏™‡∏£‡∏∏‡∏õ

### üéØ ‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô‡∏Ç‡∏≠‡∏á‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏°

1. **Hybrid Approach**:
   - CF: ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢
   - CB: ‡∏à‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏≠‡∏≤‡∏Å‡∏≤‡∏£
   - Graph: ‡∏à‡∏±‡∏ö co-occurrence patterns

2. **Scalable**:
   - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å (batch processing)
   - Fast inference (~20-50ms ‡∏ö‡∏ô CPU)

3. **Interpretable**:
   - ‡∏î‡∏π‡πÑ‡∏î‡πâ‡∏ß‡πà‡∏≤‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏™‡∏≤‡∏¢‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏£
   - Attention weights ‡∏ö‡∏≠‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå

4. **Production-Ready**:
   - FastAPI with async support
   - Error handling ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô
   - API documentation (Swagger UI)

---

### üîß ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï

1. **More Training Data**:
   - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢ ‚Üí ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á CF accuracy

2. **Text Embeddings**:
   - ‡πÉ‡∏ä‡πâ BERT/Thai-BERT ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö symptom text
   - ‡∏à‡∏±‡∏ö semantic similarity ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô

3. **Severity Modeling**:
   - ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏û‡∏£‡πâ‡∏≠‡∏° severity level

4. **Multi-modal Input**:
   - ‡∏£‡∏±‡∏ö‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û/‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö

5. **Explainability**:
   - ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡πÑ‡∏°‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ô‡∏µ‡πâ

---

### üìö ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á

- **NCF**: He et al. (2017) "Neural Collaborative Filtering"
- **GAT**: Veliƒçkoviƒá et al. (2018) "Graph Attention Networks"
- **PyTorch Geometric**: Fey & Lenssen (2019)
- **FastAPI**: Sebasti√°n Ram√≠rez

---

**‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ô‡∏µ‡πâ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏°‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î** üéì
