# Test Suite Documentation

This directory contains the test suite for the Symptom Recommendation System.

## Overview

The test suite validates the functionality of:
- API endpoints (FastAPI routes)
- Model inference and recommendation logic
- Data processing and feature engineering
- LLM integration (if API keys available)

## Test Files

### test_api.py
Tests for FastAPI endpoints and API behavior.

**Coverage:**
- `GET /health` - Health check endpoint
- `GET /symptoms` - Symptom vocabulary listing
- `POST /recommend` - Recommendation generation
- Request validation and error handling
- Response schema validation

**Key Test Cases:**
```python
test_health_endpoint()           # Verifies server health
test_symptoms_endpoint()          # Tests vocabulary retrieval
test_recommend_endpoint()         # Tests recommendation flow
test_recommend_invalid_gender()   # Validates input constraints
test_recommend_unknown_symptoms() # Handles unknown symptoms
```

### test_model.py
Tests for model architecture and inference logic.

**Coverage:**
- Model initialization and parameter counts
- Forward pass computation
- Recommendation generation
- Score fusion (CF + CB + GAT)
- Edge case handling (empty symptoms, invalid inputs)

**Key Test Cases:**
```python
test_model_initialization()      # Verifies model setup
test_forward_pass()               # Tests score computation
test_recommend_function()         # Tests recommendation logic
test_symptom_embedding_lookup()   # Validates embedding retrieval
test_graph_structure()            # Verifies GAT graph construction
```

## Running Tests

### Run All Tests
```bash
pytest
```

### Run Specific Test File
```bash
pytest tests/test_api.py
pytest tests/test_model.py
```

### Run Specific Test Function
```bash
pytest tests/test_api.py::test_health_endpoint
pytest tests/test_model.py::test_recommend_function
```

### Run with Verbose Output
```bash
pytest -v
```

### Run with Coverage Report
```bash
# Generate coverage report
pytest --cov=. --cov-report=html

# View report
open htmlcov/index.html
```

### Run with Print Statements
```bash
pytest -s
```

## Test Requirements

Tests require the following to be present:

### Model Artifacts
The following files must exist (generated by running `python train.py`):
- `model.pth` - Trained model weights
- `symptom_to_idx.pkl` - Symptom vocabulary mapping
- `idx_to_symptom.pkl` - Reverse symptom mapping
- `age_bins.pkl` - Age binning configuration
- `graph.pt` - Co-occurrence graph structure
- `model_config.pkl` - Model hyperparameters

### Data Files
- `AI symptom picker data.csv` - Training data (for integration tests)

### Environment Variables (Optional)
For LLM integration tests:
- `OPENAI_API_KEY` - OpenAI API key
- `GOOGLE_API_KEY` - Google API key

If API keys are not set, tests will skip LLM-related assertions.

## Test Structure

### API Tests (test_api.py)

```python
import pytest
from fastapi.testclient import TestClient
from app import app

client = TestClient(app)

def test_health_endpoint():
    """Verify health check returns model status"""
    response = client.get("/health")
    assert response.status_code == 200
    assert response.json()["status"] == "healthy"

def test_recommend_endpoint():
    """Test recommendation generation"""
    response = client.post("/recommend", json={
        "gender": "male",
        "age": 26,
        "symptoms": ["ไอ"],
        "top_k": 5
    })
    assert response.status_code == 200
    data = response.json()
    assert "recommendations" in data
    assert len(data["recommendations"]) == 5
```

### Model Tests (test_model.py)

```python
import pytest
import torch
from model import SymptomRecommender

def test_model_initialization():
    """Verify model initializes with correct architecture"""
    model = SymptomRecommender(
        num_symptoms=338,
        num_patients=1000,
        symptom_embed_dim=128
    )
    assert isinstance(model, SymptomRecommender)
    # Count parameters
    params = sum(p.numel() for p in model.parameters())
    assert params > 1_000_000  # Should be ~2M params

def test_recommend_function():
    """Test recommendation generation logic"""
    # Load model and artifacts
    model = load_trained_model()
    recommendations = model.recommend(
        patient_idx=0,
        gender="male",
        age=26,
        query_symptoms=["ไอ"],
        top_k=5
    )
    assert len(recommendations) == 5
    assert all(isinstance(s, str) for s in recommendations)
```

## Continuous Integration

The test suite is designed to run in CI/CD pipelines:

```yaml
# Example GitHub Actions workflow
- name: Run tests
  run: |
    pip install -r requirements.txt
    pytest --cov=. --cov-report=xml

- name: Upload coverage
  uses: codecov/codecov-action@v3
  with:
    file: ./coverage.xml
```

## Test Coverage Goals

| Component | Target Coverage |
|-----------|----------------|
| API routes | 90%+ |
| Model inference | 85%+ |
| Data processing | 80%+ |
| LLM integration | 70%+ (optional) |

Current coverage:
```bash
pytest --cov=. --cov-report=term-missing
```

## Common Issues

### Issue: Model artifacts not found
**Solution**: Run training first
```bash
python train.py
```

### Issue: Tests fail with CUDA errors
**Solution**: Force CPU mode
```bash
pytest -k "not gpu"
```

### Issue: LLM tests timeout
**Solution**: Set longer timeout or skip
```bash
pytest -k "not llm"  # Skip LLM tests
pytest --timeout=30  # Increase timeout
```

## Adding New Tests

When adding new functionality, follow this pattern:

1. **Write test first** (TDD approach)
2. **Test both success and failure cases**
3. **Mock external dependencies** (LLM APIs, etc.)
4. **Use fixtures for common setup**
5. **Add docstrings** explaining what's tested

Example test template:
```python
def test_new_feature():
    """
    Test description: what this test validates

    Given: Initial state
    When: Action performed
    Then: Expected outcome
    """
    # Arrange
    setup_code()

    # Act
    result = perform_action()

    # Assert
    assert result == expected
```

## Mocking External Services

For LLM API calls, use mocking to avoid actual API charges:

```python
from unittest.mock import patch, MagicMock

@patch('app.llm')
def test_llm_enhancement(mock_llm):
    """Test LLM enhancement without actual API call"""
    mock_response = MagicMock()
    mock_response.recommendations = ["symptom1", "symptom2"]
    mock_response.score = 0.9
    mock_response.reason = "Test reason"
    mock_llm.with_structured_output.return_value.invoke.return_value = mock_response

    # Test code here
```

## Test Data

Test data is stored in:
- `tests/fixtures/` - Mock data files
- `tests/conftest.py` - Pytest fixtures

Example fixture:
```python
# tests/conftest.py
import pytest

@pytest.fixture
def sample_symptoms():
    return ["ไอ", "เสมหะ", "เจ็บคอ"]

@pytest.fixture
def sample_patient():
    return {
        "gender": "male",
        "age": 26,
        "symptoms": ["ไอ"]
    }
```

## Performance Testing

For load testing the API:

```bash
# Install locust
pip install locust

# Run load test
locust -f tests/load_test.py --host=http://localhost:8000
```

## Reporting Issues

If tests fail:
1. Check model artifacts are present
2. Verify environment variables are set
3. Run with verbose output: `pytest -v -s`
4. Check logs in console output
5. Report issue with full error trace

---

For questions about testing, see the main [README.md](../README.md) or open an issue on GitHub.
